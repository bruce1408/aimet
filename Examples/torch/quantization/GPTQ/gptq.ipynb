{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GPTQ\n",
    "\n",
    "This notebook shows a working code example of how to use AIMET to perform post training quantization using GPTQ).\n",
    "\n",
    "GPTQ performs layer wise quantization and applies a series of optimization on the OBQ method which uses second order information for updating the weights.\n",
    "\n",
    "Using GPTQ PTQ, a model is able to achieve an accuracy closer to the FP32 model, while using low bit-width integer quantization. We observe considerable speedup compared to another PTQ method, AdaRound, while achieving similar performance.\n",
    "\n",
    "#### Overall flow\n",
    "This notebook covers the following:\n",
    "1. Define helper functions and the configurations needed to create quantsim and apply GPTQ\n",
    "2. Load the FP32 model and evaluate the model to find the baseline FP32 accuracy\n",
    "3. Create a quantization simulation model and determine quantized accuracy\n",
    "4. Apply GPTQ technique and evaluate the PPL on the quantsim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "  #### 1. Define helper functions and the configurations needed to create quantsim and apply GPTQ\n",
    "\n",
    "First define the checkpoint directory which specifies the model architecture and identifies the bblocks to be used to perform the sampling time optimization\n",
    "\n",
    "We then define the config file needed to create quantsim of the model, for GPTQ we specify the per channel quantization for params\n",
    "\n",
    "Next, we define helper functions which help to convert the Conv1D to linear layer as the current implementation applies the optimization on linear layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "root =  os.getcwd()\n",
    "gptq_config_path = os.path.join(root, 'GPTQ/quantsim_config.json')\n",
    "checkpoints_config_path= os.path.join(root, 'GPTQ/gpt2-small_checkpoints_config.json')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "def convert_conv1d_to_linear(model: torch.nn.Module) -> torch.nn.Module:\n",
    "    change_modules = {}\n",
    "    device = model.device\n",
    "    for name, module in model.named_modules():\n",
    "        #print(name, module)\n",
    "        if isinstance(module, transformers.Conv1D):\n",
    "            shapes = module.weight.shape\n",
    "            new_layer = nn.Linear(shapes[0], shapes[1]).to(device)\n",
    "            w = module.weight.clone().T\n",
    "            new_layer.weight.data.copy_(w)\n",
    "            if module.bias is not None:\n",
    "                new_layer.bias.data.copy_(module.bias)\n",
    "            change_modules[name] = new_layer\n",
    "\n",
    "    for name, new_layer in change_modules.items():\n",
    "        sublayers = name.split(\".\")\n",
    "        _module = model.get_submodule(\".\".join(sublayers[:-1]))\n",
    "        setattr(_module, sublayers[-1], new_layer)\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def get_dummy_input(data_loader, device, return_tuple=False):\n",
    "    for data in data_loader:\n",
    "        if return_tuple:\n",
    "            return data[0].to(device)\n",
    "        else:\n",
    "            inputs = dict()\n",
    "            inputs['input'] = data[0].to(device)\n",
    "            return inputs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define the class to construct us the dataset and the evaluate function.\n",
    "\n",
    "We take inspiration from the huggingface implementation of perplexity evaluation\n",
    "\n",
    "https://huggingface.co/docs/transformers/perplexity\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from aimet_torch.utils import get_all_quantizers, in_eval_mode\n",
    "import random\n",
    "\n",
    "class WikiTextDataPipeline:\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        input_batch = torch.stack([item[0] for item in batch])\n",
    "        target_batch = torch.stack([item[1] for item in batch])\n",
    "        input_batch = input_batch.squeeze(dim=1)\n",
    "        target_batch = target_batch.squeeze(dim=1)\n",
    "        return input_batch, target_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_train_dataloader(model: torch.nn.Module, batch_size = 1, nsamples=128) -> torch.utils.data.DataLoader:\n",
    "        traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "        trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n",
    "        model_act = GPT2LMHeadModel.from_pretrained(model,  cache_dir='/local/mnt/workspace/juhimitt/remote_dev/aimet_main')\n",
    "        max_length = model_act.config.n_positions\n",
    "        trainloader=[]\n",
    "        for _ in range(nsamples):\n",
    "            i=random.randint(0, trainenc.input_ids.shape[1]-max_length-1)\n",
    "            j=i+max_length\n",
    "            inp = trainenc.input_ids[:,i:j]\n",
    "            tar = inp.clone()\n",
    "            tar[:,:-1]=-100\n",
    "            trainloader.append((inp,tar))\n",
    "\n",
    "        print(f'length of the train loader is {len(trainloader)}')\n",
    "        data_loader = DataLoader(trainloader, batch_size=batch_size, collate_fn=WikiTextDataPipeline.collate_fn)\n",
    "        return data_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def get_val_dataloader(model: torch.nn.Module, batch_size=1, stride=512) ->  torch.utils.data.DataLoader:\n",
    "        testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "        testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "        testloader = []\n",
    "\n",
    "        max_len = testenc.input_ids.size(1)\n",
    "        prev_end_loc = 0\n",
    "        model_act = GPT2LMHeadModel.from_pretrained(model, cache_dir='/local/mnt/workspace/juhimitt/remote_dev/aimet_main')\n",
    "        max_length = model_act.config.n_positions\n",
    "        for begin_loc in tqdm(range(0, max_len, stride)):\n",
    "            end_loc = min(begin_loc + max_length, max_len)\n",
    "            trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "            input_ids = testenc.input_ids[:, begin_loc:end_loc]\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            testloader.append((input_ids, target_ids))\n",
    "            prev_end_loc = end_loc\n",
    "            if end_loc == max_len:\n",
    "                break\n",
    "        data_loader = DataLoader(testloader, batch_size=batch_size, collate_fn=WikiTextDataPipeline.collate_fn)\n",
    "        return data_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_model(model: torch.nn.Module, data_loader) -> float:\n",
    "        model.config.return_dict = False\n",
    "        model.config.return_past = False\n",
    "        nlls = []\n",
    "        with torch.no_grad(), in_eval_mode(model):\n",
    "            for idx, data in enumerate(data_loader):\n",
    "                if idx%5==0:\n",
    "                    print(f'{idx} \\n')\n",
    "                device = model.device\n",
    "                predictions = model(data[0].to(device), labels=data[1].to(device))\n",
    "                neg_log_likelihood, *_ = predictions\n",
    "                nlls.append(neg_log_likelihood)\n",
    "\n",
    "        ppl = torch.exp(torch.stack(nlls).mean())\n",
    "        return ppl\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 2. Load the model and evaluate to get a baseline FP32 perplexity score\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir='/local/mnt/workspace/juhimitt/remote_dev/aimet_main').to(device)\n",
    "model  = convert_conv1d_to_linear(model)\n",
    "model = model.eval()\n",
    "val_loader = WikiTextDataPipeline.get_val_dataloader(model_name)\n",
    "fp32_score = WikiTextDataPipeline.evaluate_model(model, val_loader)\n",
    "print(f'full precision model score is {fp32_score}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 3. Create a quantization simulation model and determine quantized perplexity score\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from aimet_torch.qc_quantize_op import QcQuantizeWrapper\n",
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_torch.adaround.adaround_weight import Adaround, AdaroundParameters\n",
    "\n",
    "\n",
    "model_name = 'gpt2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir='/local/mnt/workspace/juhimitt/remote_dev/aimet_main').to(device)\n",
    "model  = convert_conv1d_to_linear(model)\n",
    "model.config.return_dict = False\n",
    "model.config.return_past = False\n",
    "model = model.eval()\n",
    "\n",
    "val_loader = WikiTextDataPipeline.get_val_dataloader(model_name)\n",
    "dummy_input = get_dummy_input(val_loader, device, return_tuple=True)\n",
    "\n",
    "quant_sim = QuantizationSimModel(model, dummy_input=dummy_input,\n",
    "                                     default_param_bw=4,\n",
    "                                     config_file=gptq_config_path)\n",
    "\n",
    "# we disable the quantizers for all the non linear layers\n",
    "for _, wrapper in quant_sim.model.named_modules():\n",
    "\n",
    "    if not isinstance(wrapper, QcQuantizeWrapper):\n",
    "        continue\n",
    "        # pylint: disable=protected-access\n",
    "    if not isinstance(wrapper._module_to_wrap, torch.nn.Linear):\n",
    "\n",
    "        param_quantizers, input_quantizers, output_quantizers = get_all_quantizers(wrapper)\n",
    "\n",
    "        for q in param_quantizers + input_quantizers + output_quantizers:\n",
    "            q.enabled = False\n",
    "\n",
    "ignore_quant_ops_list=[model.lm_head]\n",
    "Adaround._exclude_modules(model, quant_sim, ignore_quant_ops_list)\n",
    "\n",
    "quantsim_score = WikiTextDataPipeline.evaluate_model(quant_sim.model, val_loader)\n",
    "print(f'Quant sim score is {quantsim_score}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 4. Apply GPTQ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "first we create the model, now we are defining relevant paramters and output directory path to store the updated weigts model and the encodings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from aimet_torch.GPTQ.gptq_weight import GPTQ, GPTQParameters\n",
    "import os\n",
    "\n",
    "model_name = 'gpt2'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir='/local/mnt/workspace/juhimitt/remote_dev/aimet_main').to(device)\n",
    "model  = convert_conv1d_to_linear(model)\n",
    "model.config.return_dict = False\n",
    "model.config.return_past = False\n",
    "model = model.eval()\n",
    "\n",
    "def forward_fn(model, batch):\n",
    "    inputs = dict()\n",
    "    inputs['inputs'] = batch[0].to(device)\n",
    "    output = model(inputs['inputs'])\n",
    "    return output\n",
    "\n",
    "num_batches = 128\n",
    "block_size = 128\n",
    "percdamp = 0.01\n",
    "reordering = False\n",
    "\n",
    "train_loader = WikiTextDataPipeline.get_train_dataloader(model_name)\n",
    "val_loader = WikiTextDataPipeline.get_val_dataloader(model_name)\n",
    "\n",
    "dummy_input = get_dummy_input(val_loader, device, return_tuple=True)\n",
    "\n",
    "\n",
    "params = GPTQParameters(\n",
    "        data_loader=train_loader,\n",
    "        num_batches=num_batches,\n",
    "        block_size = block_size,\n",
    "        forward_fn=forward_fn,\n",
    "        percdamp = percdamp,\n",
    "        reordering = reordering\n",
    "    )\n",
    "\n",
    "output_dir = './'\n",
    "output_path = os.path.join(output_dir, \"gptq\")\n",
    "fname = os.path.join(output_path, \"gptq_model.pt\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gptq_model = GPTQ.apply_gptq_with_cache(\n",
    "         model, dummy_input,\n",
    "         path=output_path,\n",
    "         filename_prefix=\"parameter\",\n",
    "         params=params,\n",
    "         default_param_bw=4,\n",
    "         default_quant_scheme='tf',\n",
    "         default_config_file=gptq_config_path,\n",
    "         ignore_quant_ops_list=[model.lm_head],\n",
    "         checkpoints_config=checkpoints_config_path\n",
    "     )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "the returned gptq model has the updated weights, we then create a quantsim on this with the initial embeddings that we started with and then evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quant_sim = QuantizationSimModel(gptq_model, dummy_input=dummy_input,\n",
    "                                     default_param_bw=4,\n",
    "                                     config_file=gptq_config_path)\n",
    "\n",
    "for _, wrapper in quant_sim.model.named_modules():\n",
    "\n",
    "    if not isinstance(wrapper, QcQuantizeWrapper):\n",
    "        continue\n",
    "    # pylint: disable=protected-access\n",
    "    if not isinstance(wrapper._module_to_wrap, torch.nn.Linear):\n",
    "\n",
    "        param_quantizers, input_quantizers, output_quantizers = get_all_quantizers(wrapper)\n",
    "\n",
    "        for q in param_quantizers + input_quantizers + output_quantizers:\n",
    "            q.enabled = False\n",
    "Adaround._exclude_modules(gptq_model, quant_sim, [gptq_model.lm_head])\n",
    "quant_sim.set_and_freeze_param_encodings(encoding_path='./gptq/parameter.encodings')\n",
    "model = quant_sim.model\n",
    "\n",
    "gptq_score = WikiTextDataPipeline.evaluate_model(model, val_loader)\n",
    "print(f'gptq score is {gptq_score}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
